{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import shutil\n",
    "import pickle\n",
    "import sqlite3\n",
    "from collections import OrderedDict, Counter\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def with_path(p):\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    return os.path.join(current_dir, p)\n",
    "\n",
    "DICTIONARY_PATH = 'db/dictionary.json'\n",
    "EOS = '<eos>'\n",
    "UNK = '<unk>'\n",
    "PAD = '<pad>'\n",
    "GO = '<go>'\n",
    "\n",
    "buckets = [\n",
    "    (5, 15)\n",
    "    , (10, 20)\n",
    "    , (15, 25)\n",
    "    , (50, 100)\n",
    "]\n",
    "\n",
    "def time(s):\n",
    "    ret = ''\n",
    "    if s >= 60 * 60:\n",
    "        h = math.floor(s / (60 * 60))\n",
    "        ret += '{}h'.format(h)\n",
    "        s -= h * 60 * 60\n",
    "    if s >= 60:\n",
    "        m = math.floor(s / 60)\n",
    "        ret += '{}m'.format(m)\n",
    "        s -= m * 60\n",
    "    if s >= 1:\n",
    "        s = math.floor(s)\n",
    "        ret += '{}s'.format(s)\n",
    "    return ret\n",
    "\n",
    "def load_dictionary():\n",
    "    with open(with_path(DICTIONARY_PATH), 'r', encoding='UTF-8') as fp:\n",
    "        dictionary = [EOS, UNK, PAD, GO] + json.load(fp)\n",
    "        index_word = OrderedDict()\n",
    "        word_index = OrderedDict()\n",
    "        for index, word in enumerate(dictionary):\n",
    "            index_word[index] = word\n",
    "            word_index[word] = index\n",
    "        dim = len(dictionary)\n",
    "    return dim, dictionary, index_word, word_index\n",
    "\n",
    "\n",
    "dim, dictionary, index_word, word_index = load_dictionary()\n",
    "\n",
    "print('dim: ', dim)\n",
    "\n",
    "EOS_ID = word_index[EOS]\n",
    "UNK_ID = word_index[UNK]\n",
    "PAD_ID = word_index[PAD]\n",
    "GO_ID = word_index[GO]\n",
    "\n",
    "\n",
    "# 数据库数据处理\n",
    "class BucketData(object):\n",
    "\n",
    "    def __init__(self, buckets_dir, encoder_size, decoder_size):\n",
    "        self.encoder_size = encoder_size\n",
    "        self.decoder_size = decoder_size\n",
    "        self.name = 'bucket_%d_%d.db' % (encoder_size, decoder_size)\n",
    "        self.path = os.path.join(buckets_dir, self.name)\n",
    "        self.conn = sqlite3.connect(self.path)\n",
    "        self.cur = self.conn.cursor()\n",
    "        sql = '''SELECT MAX(ROWID) FROM conversation;'''\n",
    "        self.size = self.cur.execute(sql).fetchall()[0][0]\n",
    "\n",
    "    def all_answers(self, ask):\n",
    "        \"\"\"找出所有数据库中符合ask的answer\n",
    "        \"\"\"\n",
    "        sql = '''\n",
    "        SELECT answer FROM conversation\n",
    "        WHERE ask = '{}';\n",
    "        '''.format(ask.replace(\"'\", \"''\"))\n",
    "        ret = []\n",
    "        for s in self.cur.execute(sql):\n",
    "            ret.append(s[0])\n",
    "        return list(set(ret))\n",
    "\n",
    "    def random(self):\n",
    "        while True:\n",
    "            # 选择一个[1, MAX(ROWID)]中的整数，读取这一行\n",
    "            rowid = np.random.randint(1, self.size + 1)\n",
    "            sql = '''\n",
    "            SELECT ask, answer FROM conversation\n",
    "            WHERE ROWID = {};\n",
    "            '''.format(rowid)\n",
    "            ret = self.cur.execute(sql).fetchall()\n",
    "            if len(ret) == 1:\n",
    "                ask, answer = ret[0]\n",
    "                if ask is not None and answer is not None:\n",
    "                    return ask, answer\n",
    "\n",
    "# 读数据\n",
    "def read_bucket_dbs(buckets_dir):\n",
    "    ret = []\n",
    "    for encoder_size, decoder_size in buckets:\n",
    "        bucket_data = BucketData(buckets_dir, encoder_size, decoder_size)\n",
    "        ret.append(bucket_data)\n",
    "    return ret\n",
    "# 句子标号\n",
    "def sentence_indice(sentence):\n",
    "    ret = []\n",
    "    for  word in sentence:\n",
    "        if word in word_index:\n",
    "            ret.append(word_index[word])\n",
    "        else:\n",
    "            ret.append(word_index[UNK])\n",
    "    return ret\n",
    "\n",
    "def indice_sentence(indice):\n",
    "    ret = []\n",
    "    for index in indice:\n",
    "        word = index_word[index]\n",
    "        if word == EOS:\n",
    "            break\n",
    "        if word != UNK and word != GO and word != PAD:\n",
    "            ret.append(word)\n",
    "    return ''.join(ret)\n",
    "\n",
    "# 根据词向量转换成句子\n",
    "def vector_sentence(vector):\n",
    "    return indice_sentence(vector.argmax(axis=1))\n",
    "\n",
    "# 生成数据库文件\n",
    "def generate_bucket_dbs(\n",
    "        input_dir,\n",
    "        output_dir,\n",
    "        buckets,\n",
    "        tolerate_unk=1\n",
    "    ):\n",
    "    pool = {}\n",
    "    word_count = Counter()\n",
    "    def _get_conn(key):\n",
    "        if key not in pool:\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            name = 'bucket_%d_%d.db' % key\n",
    "            path = os.path.join(output_dir, name)\n",
    "            conn = sqlite3.connect(path)\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS conversation (ask text, answer text);\"\"\")\n",
    "            conn.commit()\n",
    "            pool[key] = (conn, cur)\n",
    "        return pool[key]\n",
    "    all_inserted = {}\n",
    "    for encoder_size, decoder_size in buckets:\n",
    "        key = (encoder_size, decoder_size)\n",
    "        all_inserted[key] = 0\n",
    "    # 从input_dir列出数据库列表\n",
    "    db_paths = []\n",
    "    for dirpath, _, filenames in os.walk(input_dir):\n",
    "        for filename in (x for x in sorted(filenames) if x.endswith('.db')):\n",
    "            db_path = os.path.join(dirpath, filename)\n",
    "            db_paths.append(db_path)\n",
    "    # 对数据库列表中的数据库挨个提取\n",
    "    for db_path in db_paths:\n",
    "        print('读取数据库: {}'.format(db_path))\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        c = conn.cursor()\n",
    "        def is_valid(s):\n",
    "            unk = 0\n",
    "            for w in s:\n",
    "                if w not in word_index:\n",
    "                    unk += 1\n",
    "                    if unk > tolerate_unk:\n",
    "                        return False\n",
    "            return True\n",
    "        # 读取最大的rowid，如果rowid是连续的，结果就是里面的数据条数\n",
    "        # 比SELECT COUNT(1)要快\n",
    "        total = c.execute('''SELECT MAX(ROWID) FROM conversation;''').fetchall()[0][0]\n",
    "        ret = c.execute('''SELECT ask, answer FROM conversation;''')\n",
    "        wait_insert = []\n",
    "        def _insert(wait_insert):\n",
    "            if len(wait_insert) > 0:\n",
    "                for encoder_size, decoder_size, ask, answer in wait_insert:\n",
    "                    key = (encoder_size, decoder_size)\n",
    "                    conn, cur = _get_conn(key)\n",
    "                    cur.execute(\"\"\"\n",
    "                    INSERT INTO conversation (ask, answer) VALUES ('{}', '{}');\n",
    "                    \"\"\".format(ask.replace(\"'\", \"''\"), answer.replace(\"'\", \"''\")))\n",
    "                    all_inserted[key] += 1\n",
    "                for conn, _ in pool.values():\n",
    "                    conn.commit()\n",
    "                wait_insert = []\n",
    "            return wait_insert\n",
    "        for ask, answer in tqdm(ret, total=total):\n",
    "            if is_valid(ask) and is_valid(answer):\n",
    "                for i in range(len(buckets)):\n",
    "                    encoder_size, decoder_size = buckets[i]\n",
    "                    if len(ask) <= encoder_size and len(answer) < decoder_size:\n",
    "                        word_count.update(list(ask))\n",
    "                        word_count.update(list(answer))\n",
    "                        wait_insert.append((encoder_size, decoder_size, ask, answer))\n",
    "                        if len(wait_insert) > 10000000:\n",
    "                            wait_insert = _insert(wait_insert)\n",
    "                        break\n",
    "    word_count_arr = [(k, v) for k, v in word_count.items()]\n",
    "    word_count_arr = sorted(word_count_arr, key=lambda x: x[1], reverse=True)\n",
    "    wait_insert = _insert(wait_insert)\n",
    "    return all_inserted, word_count_arr\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('generate bucket dbs')\n",
    "    # 来源数据库目录\n",
    "    db_path = ''\n",
    "    if len(sys.argv) >= 2 and os.path.exists(sys.argv[1]):\n",
    "        db_path = sys.argv[1]\n",
    "        if not os.path.isdir(db_path):\n",
    "            print('invalid db source path, not dir')\n",
    "            exit(1)\n",
    "    elif os.path.exists('./db'):\n",
    "        db_path = './db'\n",
    "    else:\n",
    "        print('invalid db source path')\n",
    "        exit(1)\n",
    "\n",
    "    # 输出目录\n",
    "    target_path = './bucket_dbs'\n",
    "    # 不存在就建\n",
    "    if not os.path.exists(target_path):\n",
    "        os.makedirs(target_path)\n",
    "    elif os.path.exists(target_path) and not os.path.isdir(target_path):\n",
    "        print('invalid target path, exists but not dir')\n",
    "        exit(1)\n",
    "    elif os.path.exists(target_path) and os.path.isdir(target_path):\n",
    "        shutil.rmtree(target_path)\n",
    "        os.makedirs(target_path)\n",
    "\n",
    "    # 生成\n",
    "    all_inserted, word_count_arr = generate_bucket_dbs(\n",
    "        db_path,\n",
    "        target_path,\n",
    "        buckets,\n",
    "        1\n",
    "    )\n",
    "    # 导出字典\n",
    "    # print('一共找到{}个词'.format(len(word_count_arr)))\n",
    "    # with open('dictionary_detail.json', 'w') as fp:\n",
    "    #     json.dump(word_count_arr, fp, indent=4, ensure_ascii=False)\n",
    "    # with open('dictionary.json', 'w') as fp:\n",
    "    #     json.dump([x for x, _ in word_count_arr], fp, indent=4, ensure_ascii=False)\n",
    "    # 输出词库状况\n",
    "    for key, inserted_count in all_inserted.items():\n",
    "        print(key)\n",
    "        print(inserted_count)\n",
    "    print('done')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
