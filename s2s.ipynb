{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "colab": {
      "name": "s2s.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "G4f9Vka0hUdH",
        "outputId": "78455b4b-117c-41ca-fe66-19a86139d704"
      },
      "source": [
        "!pip install ipynb\n",
        "import pdb\n",
        "import random\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "#import data_utils\n",
        "\n",
        "import  ipynb.fs.full.data_utils  \n",
        "# Seq2Seq模型\n",
        "class S2SModel(object):\n",
        "#     模型参数初始化\n",
        "    def __init__(self,\n",
        "                source_vocab_size,\n",
        "                target_vocab_size,\n",
        "                buckets,\n",
        "                size,\n",
        "                dropout,\n",
        "                num_layers,\n",
        "                max_gradient_norm,\n",
        "                batch_size,\n",
        "                learning_rate,\n",
        "                num_samples,\n",
        "                forward_only=False,\n",
        "                dtype=tf.float32):\n",
        "        # init member variales\n",
        "        self.source_vocab_size = source_vocab_size\n",
        "        self.target_vocab_size = target_vocab_size\n",
        "        self.buckets = buckets\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # LSTM cells\n",
        "        cell = tf.contrib.rnn.BasicLSTMCell(size)\n",
        "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=dropout)\n",
        "        cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)\n",
        "\n",
        "        output_projection = None\n",
        "        softmax_loss_function = None\n",
        "\n",
        "        # 如果vocabulary太大，我们还是按照vocabulary来sample的话，内存会爆\n",
        "        if num_samples > 0 and num_samples < self.target_vocab_size:\n",
        "            print('开启投影：{}'.format(num_samples))\n",
        "            w_t = tf.get_variable(\n",
        "                \"proj_w\",\n",
        "                [self.target_vocab_size, size],\n",
        "                dtype=dtype\n",
        "            )\n",
        "            w = tf.transpose(w_t)\n",
        "            b = tf.get_variable(\n",
        "                \"proj_b\",\n",
        "                [self.target_vocab_size],\n",
        "                dtype=dtype\n",
        "            )\n",
        "            output_projection = (w, b)\n",
        "#             计算Cost\n",
        "            def sampled_loss(labels, logits):\n",
        "                labels = tf.reshape(labels, [-1, 1])\n",
        "                local_w_t = tf.cast(w_t, tf.float32)\n",
        "                local_b = tf.cast(b, tf.float32)\n",
        "                local_inputs = tf.cast(logits, tf.float32)\n",
        "                return tf.cast(\n",
        "                    tf.nn.sampled_softmax_loss(\n",
        "                        weights=local_w_t,\n",
        "                        biases=local_b,\n",
        "                        labels=labels,\n",
        "                        inputs=local_inputs,\n",
        "                        num_sampled=num_samples,\n",
        "                        num_classes=self.target_vocab_size\n",
        "                    ),\n",
        "                    dtype\n",
        "                )\n",
        "            softmax_loss_function = sampled_loss\n",
        "\n",
        "#         调用TensorFlow返回内置的带有attention机制的seq2seq模型\n",
        "        # seq2seq_f\n",
        "        def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n",
        "            tmp_cell = copy.deepcopy(cell)\n",
        "            return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
        "                encoder_inputs,\n",
        "                decoder_inputs,\n",
        "                tmp_cell,\n",
        "                num_encoder_symbols=source_vocab_size,\n",
        "                num_decoder_symbols=target_vocab_size,\n",
        "                embedding_size=size,\n",
        "                output_projection=output_projection,\n",
        "                feed_previous=do_decode,\n",
        "                dtype=dtype\n",
        "            )\n",
        "\n",
        "        # inputs\n",
        "        self.encoder_inputs = []\n",
        "        self.decoder_inputs = []\n",
        "        self.decoder_weights = []\n",
        "\n",
        "        # buckets中的最后一个是最大的（即第“-1”个）\n",
        "        for i in range(buckets[-1][0]):\n",
        "            self.encoder_inputs.append(tf.placeholder(\n",
        "                tf.int32,\n",
        "                shape=[None],\n",
        "                name='encoder_input_{}'.format(i)\n",
        "            ))\n",
        "        # 输出比输入大 1，这是为了保证下面的targets可以向左shift 1位\n",
        "        for i in range(buckets[-1][1] + 1):\n",
        "            self.decoder_inputs.append(tf.placeholder(\n",
        "                tf.int32,\n",
        "                shape=[None],\n",
        "                name='decoder_input_{}'.format(i)\n",
        "            ))\n",
        "            self.decoder_weights.append(tf.placeholder(\n",
        "                dtype,\n",
        "                shape=[None],\n",
        "                name='decoder_weight_{}'.format(i)\n",
        "            ))\n",
        "\n",
        "        targets = [\n",
        "            self.decoder_inputs[i + 1] for i in range(buckets[-1][1])\n",
        "        ]\n",
        "\n",
        "        if forward_only:\n",
        "            self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n",
        "                self.encoder_inputs,\n",
        "                self.decoder_inputs,\n",
        "                targets,\n",
        "                self.decoder_weights,\n",
        "                buckets,\n",
        "                lambda x, y: seq2seq_f(x, y, True),\n",
        "                softmax_loss_function=softmax_loss_function\n",
        "            )\n",
        "            if output_projection is not None:\n",
        "                for b in range(len(buckets)):\n",
        "                    self.outputs[b] = [\n",
        "                        tf.matmul(\n",
        "                            output,\n",
        "                            output_projection[0]\n",
        "                        ) + output_projection[1]\n",
        "                        for output in self.outputs[b]\n",
        "                    ]\n",
        "        else:\n",
        "            self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(\n",
        "                self.encoder_inputs,\n",
        "                self.decoder_inputs,\n",
        "                targets,\n",
        "                self.decoder_weights,\n",
        "                buckets,\n",
        "                lambda x, y: seq2seq_f(x, y, False),\n",
        "                softmax_loss_function=softmax_loss_function\n",
        "            )\n",
        "\n",
        "        params = tf.trainable_variables()\n",
        "        opt = tf.train.AdamOptimizer(\n",
        "            learning_rate=learning_rate\n",
        "        )\n",
        "\n",
        "        if not forward_only:\n",
        "            self.gradient_norms = []\n",
        "            self.updates = []\n",
        "            for output, loss in zip(self.outputs, self.losses):\n",
        "                gradients = tf.gradients(loss, params)\n",
        "                clipped_gradients, norm = tf.clip_by_global_norm(\n",
        "                    gradients,\n",
        "                    max_gradient_norm\n",
        "                )\n",
        "                self.gradient_norms.append(norm)\n",
        "                self.updates.append(opt.apply_gradients(\n",
        "                    zip(clipped_gradients, params)\n",
        "                ))\n",
        "        # self.saver = tf.train.Saver(tf.all_variables())\n",
        "        self.saver = tf.train.Saver(\n",
        "            tf.all_variables(),\n",
        "            write_version=tf.train.SaverDef.V2\n",
        "        )\n",
        "\n",
        "    def step(\n",
        "        self,\n",
        "        session,\n",
        "        encoder_inputs,\n",
        "        decoder_inputs,\n",
        "        decoder_weights,\n",
        "        bucket_id,\n",
        "        forward_only\n",
        "    ):\n",
        "        encoder_size, decoder_size = self.buckets[bucket_id]\n",
        "        if len(encoder_inputs) != encoder_size:\n",
        "            raise ValueError(\n",
        "                \"Encoder length must be equal to the one in bucket,\"\n",
        "                \" %d != %d.\" % (len(encoder_inputs), encoder_size)\n",
        "            )\n",
        "        if len(decoder_inputs) != decoder_size:\n",
        "            raise ValueError(\n",
        "                \"Decoder length must be equal to the one in bucket,\"\n",
        "                \" %d != %d.\" % (len(decoder_inputs), decoder_size)\n",
        "            )\n",
        "        if len(decoder_weights) != decoder_size:\n",
        "            raise ValueError(\n",
        "                \"Weights length must be equal to the one in bucket,\"\n",
        "                \" %d != %d.\" % (len(decoder_weights), decoder_size)\n",
        "            )\n",
        "\n",
        "        input_feed = {}\n",
        "        for i in range(encoder_size):\n",
        "            input_feed[self.encoder_inputs[i].name] = encoder_inputs[i]\n",
        "        for i in range(decoder_size):\n",
        "            input_feed[self.decoder_inputs[i].name] = decoder_inputs[i]\n",
        "            input_feed[self.decoder_weights[i].name] = decoder_weights[i]\n",
        "\n",
        "        # 理论上decoder inputs和decoder target都是n位\n",
        "        # 但是实际上decoder inputs分配了n+1位空间\n",
        "        # 不过inputs是第[0, n)，而target是[1, n+1)，刚好错开一位\n",
        "        # 最后这一位是没东西的，所以要补齐最后一位，填充0\n",
        "        last_target = self.decoder_inputs[decoder_size].name\n",
        "        input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n",
        "\n",
        "        if not forward_only:\n",
        "            output_feed = [\n",
        "                self.updates[bucket_id],\n",
        "                self.gradient_norms[bucket_id],\n",
        "                self.losses[bucket_id]\n",
        "            ]\n",
        "            output_feed.append(self.outputs[bucket_id][i])\n",
        "        else:\n",
        "            output_feed = [self.losses[bucket_id]]\n",
        "            for i in range(decoder_size):\n",
        "                output_feed.append(self.outputs[bucket_id][i])\n",
        "\n",
        "        outputs = session.run(output_feed, input_feed)\n",
        "        if not forward_only:\n",
        "            return outputs[1], outputs[2], outputs[3:]\n",
        "        else:\n",
        "            return None, outputs[0], outputs[1:]\n",
        "#     从服务器读取数据\n",
        "    def get_batch_data(self, bucket_dbs, bucket_id):\n",
        "        data = []\n",
        "        data_in = []\n",
        "        bucket_db = bucket_dbs[bucket_id]\n",
        "        for _ in range(self.batch_size):\n",
        "            ask, answer = bucket_db.random()\n",
        "            data.append((ask, answer))\n",
        "            data_in.append((answer, ask))\n",
        "        return data, data_in\n",
        "#     获得批处理数据\n",
        "    def get_batch(self, bucket_dbs, bucket_id, data):\n",
        "        encoder_size, decoder_size = self.buckets[bucket_id]\n",
        "        # bucket_db = bucket_dbs[bucket_id]\n",
        "        encoder_inputs, decoder_inputs = [], []\n",
        "        for encoder_input, decoder_input in data:\n",
        "            # encoder_input, decoder_input = random.choice(data[bucket_id])\n",
        "            # encoder_input, decoder_input = bucket_db.random()\n",
        "            encoder_input = data_utils.sentence_indice(encoder_input)\n",
        "            decoder_input = data_utils.sentence_indice(decoder_input)\n",
        "            # Encoder\n",
        "            encoder_pad = [data_utils.PAD_ID] * (\n",
        "                encoder_size - len(encoder_input)\n",
        "            )\n",
        "            encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n",
        "            # Decoder\n",
        "            decoder_pad_size = decoder_size - len(decoder_input) - 2\n",
        "            decoder_inputs.append(\n",
        "                [data_utils.GO_ID] + decoder_input +\n",
        "                [data_utils.EOS_ID] +\n",
        "                [data_utils.PAD_ID] * decoder_pad_size\n",
        "            )\n",
        "        batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n",
        "        # batch encoder\n",
        "        for i in range(encoder_size):\n",
        "            batch_encoder_inputs.append(np.array(\n",
        "                [encoder_inputs[j][i] for j in range(self.batch_size)],\n",
        "                dtype=np.int32\n",
        "            ))\n",
        "        # batch decoder\n",
        "        for i in range(decoder_size):\n",
        "            batch_decoder_inputs.append(np.array(\n",
        "                [decoder_inputs[j][i] for j in range(self.batch_size)],\n",
        "                dtype=np.int32\n",
        "            ))\n",
        "            batch_weight = np.ones(self.batch_size, dtype=np.float32)\n",
        "            for j in range(self.batch_size):\n",
        "                if i < decoder_size - 1:\n",
        "                    target = decoder_inputs[j][i + 1]\n",
        "                if i == decoder_size - 1 or target == data_utils.PAD_ID:\n",
        "                    batch_weight[j] = 0.0\n",
        "            batch_weights.append(batch_weight)\n",
        "        return batch_encoder_inputs, batch_decoder_inputs, batch_weights\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipynb in /usr/local/lib/python3.7/dist-packages (0.5.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-e20e1f21dfda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#import data_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m  \u001b[0mipynb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m# Seq2Seq模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mS2SModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ipynb.fs.full.data_utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}