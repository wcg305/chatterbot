{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import data_utils\n",
    "import s2s_model\n",
    "# TensorFlow初始化参数定义\n",
    "tf.compat.v1.app.flags.DEFINE_float(\n",
    "    'learning_rate',\n",
    "    0.0003,\n",
    "    '学习率'\n",
    ")\n",
    "tf.compat.v1.app.flags.DEFINE_float(\n",
    "    'max_gradient_norm',\n",
    "    5.0,\n",
    "    '梯度最大阈值'\n",
    ")\n",
    "tf.compat.v1.app.flags.DEFINE_float(\n",
    "    'dropout',\n",
    "    1.0,\n",
    "    '每层输出DROPOUT的大小'\n",
    ")\n",
    "tf.compat.v1.app.flags.DEFINE_integer(\n",
    "    'batch_size',\n",
    "    64,\n",
    "    '批量梯度下降的批量大小'\n",
    ")\n",
    "tf.compat.v1.app.flags.DEFINE_integer(\n",
    "    'size',\n",
    "    512,\n",
    "    'LSTM每层神经元数量'\n",
    ")\n",
    "tf.compat.v1.app.flags.DEFINE_integer(\n",
    "    'num_layers',\n",
    "    2,\n",
    "    'LSTM的层数'\n",
    ")\n",
    "tf.compat.v1.app.flags.DEFINE_integer(\n",
    "    'num_epoch',\n",
    "    5,\n",
    "    '训练几轮'\n",
    ")\n",
    "tf.compat.v1.app.flags.DEFINE_integer(\n",
    "    'num_samples',\n",
    "    512,\n",
    "    '分批softmax的样本量'\n",
    ")\n",
    "tf.compat.v1.app.flags.DEFINE_integer(\n",
    "    'num_per_epoch',\n",
    "    500000,\n",
    "    '每轮训练多少随机样本'\n",
    ")\n",
    "tf.compat.v1.app.flags.DEFINE_string(\n",
    "    'buckets_dir',\n",
    "    './bucket_dbs',\n",
    "    'sqlite3数据库所在文件夹'\n",
    ")\n",
    "tf.compat.v1.app.flags.DEFINE_string(\n",
    "    'model_dir',\n",
    "    './model',\n",
    "    '模型保存的目录'\n",
    ")\n",
    "tf.compat.v1.app.flags.DEFINE_string(\n",
    "    'model_name',\n",
    "    'model',\n",
    "    '模型保存的名称'\n",
    ")\n",
    "tf.compat.v1.app.flags.DEFINE_boolean(\n",
    "    'use_fp16',\n",
    "    False,\n",
    "    '是否使用16位浮点数（默认32位）'\n",
    ")\n",
    "tf.compat.v1.app.flags.DEFINE_integer(\n",
    "    'bleu',\n",
    "    -1,\n",
    "    '是否测试bleu'\n",
    ")\n",
    "tf.compat.v1.app.flags.DEFINE_boolean(\n",
    "    'test',\n",
    "    False,\n",
    "    '是否在测试'\n",
    ")\n",
    "\n",
    "FLAGS = tf.compat.v1.app.flags.FLAGS\n",
    "buckets = data_utils.buckets\n",
    "\n",
    "# 创建模型\n",
    "def create_model(session, forward_only):\n",
    "    dtype = tf.float16 if FLAGS.use_fp16 else tf.float32\n",
    "    model = s2s_model.S2SModel(\n",
    "        data_utils.dim,\n",
    "        data_utils.dim,\n",
    "        buckets,\n",
    "        FLAGS.size,\n",
    "        FLAGS.dropout,\n",
    "        FLAGS.num_layers,\n",
    "        FLAGS.max_gradient_norm,\n",
    "        FLAGS.batch_size,\n",
    "        FLAGS.learning_rate,\n",
    "        FLAGS.num_samples,\n",
    "        forward_only,\n",
    "        dtype\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 训练模型\n",
    "def train():\n",
    "    # 准备数据\n",
    "    print('准备数据')\n",
    "    bucket_dbs = data_utils.read_bucket_dbs(FLAGS.buckets_dir)\n",
    "    bucket_sizes = []\n",
    "    for i in range(len(buckets)):\n",
    "        bucket_size = bucket_dbs[i].size\n",
    "        bucket_sizes.append(bucket_size)\n",
    "        print('bucket {} 中有数据 {} 条'.format(i, bucket_size))\n",
    "    total_size = sum(bucket_sizes)\n",
    "    print('共有数据 {} 条'.format(total_size))\n",
    "    # 开始建模与训练\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        #　构建模型\n",
    "        model = create_model(sess, False)\n",
    "        # 初始化变量\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        buckets_scale = [\n",
    "            sum(bucket_sizes[:i + 1]) / total_size\n",
    "            for i in range(len(bucket_sizes))\n",
    "        ]\n",
    "        # 开始训练\n",
    "        metrics = '  '.join([\n",
    "            '\\r[{}]',\n",
    "            '{:.1f}%',\n",
    "            '{}/{}',\n",
    "            'loss={:.3f}',\n",
    "            '{}/{}'\n",
    "        ])\n",
    "        bars_max = 20\n",
    "        for epoch_index in range(1, FLAGS.num_epoch + 1):\n",
    "            print('Epoch {}:'.format(epoch_index))\n",
    "            time_start = time.time()\n",
    "            epoch_trained = 0\n",
    "            batch_loss = []\n",
    "            while True:\n",
    "                # 选择一个要训练的bucket\n",
    "                random_number = np.random.random_sample()\n",
    "                bucket_id = min([\n",
    "                    i for i in range(len(buckets_scale))\n",
    "                    if buckets_scale[i] > random_number\n",
    "                ])\n",
    "                data, data_in = model.get_batch_data(\n",
    "                    bucket_dbs,\n",
    "                    bucket_id\n",
    "                )\n",
    "                encoder_inputs, decoder_inputs, decoder_weights = model.get_batch(\n",
    "                    bucket_dbs,\n",
    "                    bucket_id,\n",
    "                    data\n",
    "                )\n",
    "                _, step_loss, output = model.step(\n",
    "                    sess,\n",
    "                    encoder_inputs,\n",
    "                    decoder_inputs,\n",
    "                    decoder_weights,\n",
    "                    bucket_id,\n",
    "                    False\n",
    "                )\n",
    "                epoch_trained += FLAGS.batch_size\n",
    "                batch_loss.append(step_loss)\n",
    "                time_now = time.time()\n",
    "                time_spend = time_now - time_start\n",
    "                time_estimate = time_spend / (epoch_trained / FLAGS.num_per_epoch)\n",
    "                percent = min(100, epoch_trained / FLAGS.num_per_epoch) * 100\n",
    "                bars = math.floor(percent / 100 * bars_max)\n",
    "                sys.stdout.write(metrics.format(\n",
    "                    '=' * bars + '-' * (bars_max - bars),\n",
    "                    percent,\n",
    "                    epoch_trained, FLAGS.num_per_epoch,\n",
    "                    np.mean(batch_loss),\n",
    "                    data_utils.time(time_spend), data_utils.time(time_estimate)\n",
    "                ))\n",
    "                sys.stdout.flush()\n",
    "                if epoch_trained >= FLAGS.num_per_epoch:\n",
    "                    break\n",
    "            print('\\n')\n",
    "\n",
    "        if not os.path.exists(FLAGS.model_dir):\n",
    "            os.makedirs(FLAGS.model_dir)\n",
    "        model.saver.save(sess, os.path.join(FLAGS.model_dir, FLAGS.model_name))\n",
    "\n",
    "# 测试模型\n",
    "def test():\n",
    "    class TestBucket(object):\n",
    "        def __init__(self, sentence):\n",
    "            self.sentence = sentence\n",
    "        def random(self):\n",
    "            return sentence, ''\n",
    "    with tf.Session() as sess:\n",
    "        #　构建模型\n",
    "        model = create_model(sess, True)\n",
    "        model.batch_size = 1\n",
    "        # 初始化变量\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        model.saver.restore(sess, os.path.join(FLAGS.model_dir, FLAGS.model_name))\n",
    "        sys.stdout.write(\"> \")\n",
    "        sys.stdout.flush()\n",
    "        sentence = sys.stdin.readline()\n",
    "        while sentence:\n",
    "            bucket_id = min([\n",
    "                b for b in range(len(buckets))\n",
    "                if buckets[b][0] > len(sentence)\n",
    "            ])\n",
    "            data, _ = model.get_batch_data(\n",
    "                {bucket_id: TestBucket(sentence)},\n",
    "                bucket_id\n",
    "            )\n",
    "            encoder_inputs, decoder_inputs, decoder_weights = model.get_batch(\n",
    "                {bucket_id: TestBucket(sentence)},\n",
    "                bucket_id,\n",
    "                data\n",
    "            )\n",
    "            _, _, output_logits = model.step(\n",
    "                sess,\n",
    "                encoder_inputs,\n",
    "                decoder_inputs,\n",
    "                decoder_weights,\n",
    "                bucket_id,\n",
    "                True\n",
    "            )\n",
    "            outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "            ret = data_utils.indice_sentence(outputs)\n",
    "            print(ret)\n",
    "            print(\"> \", end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            sentence = sys.stdin.readline()\n",
    "\n",
    "def main(_):\n",
    "    if FLAGS.test:\n",
    "        test()\n",
    "    else:\n",
    "        train()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(0)\n",
    "    tf.compat.v1.set_random_seed(0)\n",
    "    tf.compat.v1.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
